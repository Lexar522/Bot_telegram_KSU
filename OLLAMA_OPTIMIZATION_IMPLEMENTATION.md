# üîß –¢–µ—Ö–Ω—ñ—á–Ω–∞ —ñ–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó OLLAMA

## üì¶ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–æ–≤–∏—Ö –º–æ–¥—É–ª—ñ–≤

### 1. –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –∫–ª—ñ—î–Ω—Ç OLLAMA

```python
# ollama_optimized/client.py
import aiohttp
import time
from typing import Optional, Dict, List
from config import OLLAMA_API_URL, OLLAMA_MODEL
from ollama_optimized.prompt_builder import PromptBuilder
from ollama_optimized.context_optimizer import ContextOptimizer
from ollama_optimized.question_classifier import QuestionClassifier
from ollama_optimized.cache import ResponseCache
from ollama_optimized.validators.multi_level import MultiLevelValidator
from ollama_optimized.metrics.collector import MetricsCollector

class OptimizedOllamaClient:
    def __init__(self):
        self.api_url = OLLAMA_API_URL
        self.model = OLLAMA_MODEL
        self.prompt_builder = PromptBuilder()
        self.context_optimizer = ContextOptimizer()
        self.question_classifier = QuestionClassifier()
        self.cache = ResponseCache(max_size=200)
        self.validator = MultiLevelValidator()
        self.metrics = MetricsCollector()
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
        self.generation_params = {
            "factual": {
                "temperature": 0.1,
                "top_p": 0.5,
                "num_predict": 200,
                "repeat_penalty": 1.3,
                "top_k": 30
            },
            "comparison": {
                "temperature": 0.2,
                "top_p": 0.7,
                "num_predict": 600,
                "repeat_penalty": 1.5,
                "top_k": 40
            },
            "admission": {
                "temperature": 0.0,
                "top_p": 0.3,
                "num_predict": 400,
                "repeat_penalty": 1.4,
                "top_k": 20
            },
            "tuition": {
                "temperature": 0.0,
                "top_p": 0.3,
                "num_predict": 300,
                "repeat_penalty": 1.4,
                "top_k": 20
            },
            "default": {
                "temperature": 0.1,
                "top_p": 0.5,
                "num_predict": 350,
                "repeat_penalty": 1.4,
                "top_k": 30
            }
        }
    
    async def generate_response(
        self, 
        prompt: str, 
        context: List[Dict] = None,
        use_cache: bool = True
    ) -> str:
        """
        –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –∑ –∫–µ—à—É–≤–∞–Ω–Ω—è–º —Ç–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        start_time = time.time()
        
        # 1. –ö–ª–∞—Å–∏—Ñ—ñ–∫—É—î–º–æ –ø–∏—Ç–∞–Ω–Ω—è
        question_type = self.question_classifier.classify(prompt)
        
        # 2. –û—Ç—Ä–∏–º—É—î–º–æ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        from services.knowledge_service import KnowledgeService
        knowledge_service = KnowledgeService()
        full_context = knowledge_service.get_context_for_prompt(prompt)
        
        optimized_context = self.context_optimizer.optimize_context(
            prompt, 
            full_context["structured_json"]
        )
        
        # 3. –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–µ—à
        if use_cache:
            cached_response = self.cache.get(prompt, optimized_context)
            if cached_response:
                response_time = time.time() - start_time
                self.metrics.record_request(
                    prompt, cached_response, response_time, from_cache=True
                )
                return cached_response
        
        # 4. –§–æ—Ä–º—É—î–º–æ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –ø—Ä–æ–º–ø—Ç
        system_prompt = self.prompt_builder.build_system_prompt(
            question_type, 
            optimized_context
        )
        
        full_prompt = f"{system_prompt}\n\n–ü–ò–¢–ê–ù–ù–Ø: {prompt}\n\n–í–Ü–î–ü–û–í–Ü–î–¨:"
        
        # 5. –û—Ç—Ä–∏–º—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
        params = self.generation_params.get(
            question_type, 
            self.generation_params["default"]
        )
        
        # 6. –ì–µ–Ω–µ—Ä—É—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        response = await self._generate_with_retry(
            full_prompt, 
            params, 
            max_retries=3
        )
        
        # 7. –í–∞–ª—ñ–¥—É—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        validation_result = self.validator.validate(response, prompt)
        
        if not validation_result.is_valid:
            # –†–µ–≥–µ–Ω–µ—Ä—É—î–º–æ –∑ –±—ñ–ª—å—à —Å—É–≤–æ—Ä–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            params["temperature"] = 0.0
            params["top_p"] = 0.2
            response = await self._generate_with_retry(
                full_prompt, 
                params, 
                max_retries=2
            )
            
            # –ü–æ–≤—Ç–æ—Ä–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è
            validation_result = self.validator.validate(response, prompt)
        
        # 8. –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ –∫–µ—à
        if validation_result.is_valid and use_cache:
            self.cache.set(prompt, optimized_context, response)
        
        # 9. –ó–∞–ø–∏—Å—É—î–º–æ –º–µ—Ç—Ä–∏–∫–∏
        response_time = time.time() - start_time
        self.metrics.record_request(
            prompt, response, response_time, from_cache=False
        )
        
        return response
    
    async def _generate_with_retry(
        self, 
        prompt: str, 
        params: Dict, 
        max_retries: int = 3
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑ –ø–æ–≤—Ç–æ—Ä–Ω–∏–º–∏ —Å–ø—Ä–æ–±–∞–º–∏"""
        for attempt in range(max_retries):
            try:
                async with aiohttp.ClientSession() as session:
                    payload = {
                        "model": self.model,
                        "prompt": prompt,
                        "stream": False,
                        "options": params
                    }
                    
                    async with session.post(
                        f"{self.api_url}/api/generate",
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=60)
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            answer = data.get("response", "").strip()
                            if answer:
                                return answer
                        
                        if attempt < max_retries - 1:
                            await asyncio.sleep(1)  # –ó–∞—Ç—Ä–∏–º–∫–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ—é —Å–ø—Ä–æ–±–æ—é
                            
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                await asyncio.sleep(1)
        
        return "–í–∏–±–∞—á, –Ω–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å. –°–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª—é–≤–∞—Ç–∏ –ø–∏—Ç–∞–Ω–Ω—è."
```

---

### 2. –ü–æ–±—É–¥–æ–≤–∞ –ø—Ä–æ–º–ø—Ç—ñ–≤

```python
# ollama_optimized/prompt_builder.py
from typing import Dict

class PromptBuilder:
    """–ü–æ–±—É–¥–æ–≤–∞ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏—Ö –ø—Ä–æ–º–ø—Ç—ñ–≤"""
    
    # –ë–∞–∑–æ–≤—ñ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó (–∫–æ—Ä–æ—Ç–∫—ñ)
    BASE_INSTRUCTIONS = """–¢–∏ - –ø–æ–º—ñ—á–Ω–∏–∫ –∞–±—ñ—Ç—É—Ä—ñ—î–Ω—Ç–∞ –•–µ—Ä—Å–æ–Ω—Å—å–∫–æ–≥–æ –¥–µ—Ä–∂–∞–≤–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É (–•–î–£).
–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¢–Ü–õ–¨–ö–ò –ø—Ä–æ –•–î–£. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¢–Ü–õ–¨–ö–ò —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –∑ –±–∞–∑–∏ –∑–Ω–∞–Ω—å.
–ë—É–¥—å –∫–æ—Ä–æ—Ç–∫–∏–º (2-4 —Ä–µ—á–µ–Ω–Ω—è), —Ç–æ—á–Ω–∏–º, –¥—Ä—É–∂–Ω—ñ–º."""
    
    # –†–æ–ª—å —Ç–∞ –æ–±–º–µ–∂–µ–Ω–Ω—è
    ROLE_DEFINITION = """üéØ –†–û–õ–¨:
- –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¢–Ü–õ–¨–ö–ò –ø—Ä–æ –•–î–£
- –ù–Ü–ö–û–õ–ò –Ω–µ –∑–≥–∞–¥—É–π —ñ–Ω—à—ñ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç–∏
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¢–Ü–õ–¨–ö–ò –¥–∞–Ω—ñ –∑ –±–∞–∑–∏ –∑–Ω–∞–Ω—å
- –Ø–∫—â–æ –Ω–µ–º–∞—î –¥–∞–Ω–∏—Ö - —á–µ—Å–Ω–æ —Å–∫–∞–∂–∏"""
    
    # –û–±–º–µ–∂–µ–Ω–Ω—è
    CONSTRAINTS = """üö´ –ó–ê–ë–û–†–û–ù–ê:
- –•–ù–¢–£, –•–ù–£, –ö–ù–£, –õ—å–≤—ñ–≤—Å—å–∫–∏–π, –û–¥–µ—Å—å–∫–∏–π, –ë—ñ–ª–æ—Å—Ç–æ–∫, –ú—ñ—Ü–∫–µ–≤–∏—á
- –†–æ—Å—ñ–π—Å—å–∫—ñ/–∞–Ω–≥–ª—ñ–π—Å—å–∫—ñ —Å–ª–æ–≤–∞
- –í–∏–≥–∞–¥–∞–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è"""
    
    # –ü—Ä–∏–∫–ª–∞–¥–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –ø–∏—Ç–∞–Ω—å
    FEW_SHOT_EXAMPLES = {
        "factual": """
–ü–∏—Ç–∞–Ω–Ω—è: "–Ø–∫—ñ —î —Ñ–∞–∫—É–ª—å—Ç–µ—Ç–∏?"
–í—ñ–¥–ø–æ–≤—ñ–¥—å: "–í –•–î–£ —î 8 —Ñ–∞–∫—É–ª—å—Ç–µ—Ç—ñ–≤: –§–∞–∫—É–ª—å—Ç–µ—Ç —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –π —ñ–Ω–æ–∑–µ–º–Ω–æ—ó —Ñ—ñ–ª–æ–ª–æ–≥—ñ—ó, –∂—É—Ä–Ω–∞–ª—ñ—Å—Ç–∏–∫–∏ —Ç–∞ –º–∏—Å—Ç–µ—Ü—Ç–≤; –§–∞–∫—É–ª—å—Ç–µ—Ç –ø—Å–∏—Ö–æ–ª–æ–≥—ñ—ó, —ñ—Å—Ç–æ—Ä—ñ—ó —Ç–∞ —Å–æ—Ü—ñ–æ–ª–æ–≥—ñ—ó; –ú–µ–¥–∏—á–Ω–∏–π —Ñ–∞–∫—É–ª—å—Ç–µ—Ç; –§–∞–∫—É–ª—å—Ç–µ—Ç –±—ñ–æ–ª–æ–≥—ñ—ó, –≥–µ–æ–≥—Ä–∞—Ñ—ñ—ó —Ç–∞ –µ–∫–æ–ª–æ–≥—ñ—ó; –§–∞–∫—É–ª—å—Ç–µ—Ç —Ñ—ñ–∑–∏—á–Ω–æ–≥–æ –≤–∏—Ö–æ–≤–∞–Ω–Ω—è —Ç–∞ —Å–ø–æ—Ä—Ç—É; –ü–µ–¥–∞–≥–æ–≥—ñ—á–Ω–∏–π —Ñ–∞–∫—É–ª—å—Ç–µ—Ç; –§–∞–∫—É–ª—å—Ç–µ—Ç –±—ñ–∑–Ω–µ—Å—É —ñ –ø—Ä–∞–≤–∞; –§–∞–∫—É–ª—å—Ç–µ—Ç –∫–æ–º–ø'—é—Ç–µ—Ä–Ω–∏—Ö –Ω–∞—É–∫, —Ñ—ñ–∑–∏–∫–∏ —Ç–∞ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏. –û–±–µ—Ä–∏ —Ñ–∞–∫—É–ª—å—Ç–µ—Ç –¥–ª—è –ø–µ—Ä–µ–≥–ª—è–¥—É —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç–µ–π üéì"
""",
        "tuition": """
–ü–∏—Ç–∞–Ω–Ω—è: "–°–∫—ñ–ª—å–∫–∏ –∫–æ—à—Ç—É—î –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –ø—Å–∏—Ö–æ–ª–æ–≥–∞?"
–í—ñ–¥–ø–æ–≤—ñ–¥—å: "–í–∞—Ä—Ç—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –ü—Å–∏—Ö–æ–ª–æ–≥—ñ—é –≤ –•–î–£: –ë–∞–∫–∞–ª–∞–≤—Ä (–¥–µ–Ω–Ω–∞) - 3683 –≥—Ä–Ω/–º—ñ—Å—è—Ü—å, 18415 –≥—Ä–Ω/—Å–µ–º–µ—Å—Ç—Ä, 36830 –≥—Ä–Ω/—Ä—ñ–∫. –ú–∞–≥—ñ—Å—Ç—Ä (–¥–µ–Ω–Ω–∞) - 4788 –≥—Ä–Ω/–º—ñ—Å—è—Ü—å, 23940 –≥—Ä–Ω/—Å–µ–º–µ—Å—Ç—Ä, 47880 –≥—Ä–Ω/—Ä—ñ–∫. –î–ª—è —É—Ç–æ—á–Ω–µ–Ω–Ω—è: +380 552 494375 üí∞"
""",
        "admission": """
–ü–∏—Ç–∞–Ω–Ω—è: "–Ø–∫—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –¥–ª—è –≤—Å—Ç—É–ø—É?"
–í—ñ–¥–ø–æ–≤—ñ–¥—å: "–î–ª—è –≤—Å—Ç—É–ø—É –¥–æ –•–î–£ –ø–æ—Ç—Ä—ñ–±–Ω—ñ: –∑–∞—è–≤–∞, –¥–æ–∫—É–º–µ–Ω—Ç –ø—Ä–æ –æ—Å–≤—ñ—Ç—É, —Ñ–æ—Ç–æ 3x4 (4 —à—Ç.), –∫–æ–ø—ñ—è –ø–∞—Å–ø–æ—Ä—Ç–∞, –∫–æ–ø—ñ—è —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ–π–Ω–æ–≥–æ –∫–æ–¥—É, –º–µ–¥–∏—á–Ω–∞ –¥–æ–≤—ñ–¥–∫–∞ (—Ñ–æ—Ä–º–∞ 086-–æ), —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ó–ù–û. –î–µ—Ç–∞–ª—å–Ω—ñ—à–µ: +380 552 494375 üìû"
"""
    }
    
    def build_system_prompt(self, question_type: str, context: Dict) -> str:
        """–ü–æ–±—É–¥–æ–≤–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç—É"""
        # –ë–∞–∑–æ–≤—ñ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó
        prompt_parts = [self.BASE_INSTRUCTIONS, self.ROLE_DEFINITION, self.CONSTRAINTS]
        
        # –î–æ–¥–∞—î–º–æ –ø—Ä–∏–∫–ª–∞–¥–∏ –¥–ª—è —Ç–∏–ø—É –ø–∏—Ç–∞–Ω–Ω—è
        if question_type in self.FEW_SHOT_EXAMPLES:
            prompt_parts.append(f"üí° –ü–†–ò–ö–õ–ê–î–ò:\n{self.FEW_SHOT_EXAMPLES[question_type]}")
        
        # –î–æ–¥–∞—î–º–æ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_text = self._format_context(context)
        prompt_parts.append(f"üìö –ë–ê–ó–ê –ó–ù–ê–ù–¨:\n{context_text}")
        
        # –Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó –∑ —Å–∞–º–æ–ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
        prompt_parts.append(self._get_self_check_instructions())
        
        return "\n\n".join(prompt_parts)
    
    def _format_context(self, context: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥–ª—è –ø—Ä–æ–º–ø—Ç—É"""
        # –û–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        import json
        context_str = json.dumps(context, ensure_ascii=False, indent=2)
        
        # –Ø–∫—â–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–∏–π, –æ–±—Ä—ñ–∑–∞—î–º–æ
        max_context_length = 2000  # —Ç–æ–∫–µ–Ω—ñ–≤
        if len(context_str) > max_context_length:
            # –ó–∞–ª–∏—à–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –≤–∞–∂–ª–∏–≤—ñ —Å–µ–∫—Ü—ñ—ó
            important_keys = ["university", "contacts", "admission", "documents"]
            filtered_context = {
                k: v for k, v in context.items() 
                if k in important_keys or any(ik in k for ik in important_keys)
            }
            context_str = json.dumps(filtered_context, ensure_ascii=False, indent=2)
        
        return context_str
    
    def _get_self_check_instructions(self) -> str:
        """–Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó –¥–ª—è —Å–∞–º–æ–ø–µ—Ä–µ–≤—ñ—Ä–∫–∏"""
        return """‚úÖ –°–ê–ú–û–ü–ï–†–ï–í–Ü–†–ö–ê –ü–ï–†–ï–î –í–Ü–î–ü–†–ê–í–ö–û–Æ:
1. –ß–∏ –Ω–µ–º–∞—î –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–∏—Ö —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—ñ–≤?
2. –ß–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–∞ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—è?
3. –ß–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –¢–Ü–õ–¨–ö–ò –¥–∞–Ω—ñ –∑ –±–∞–∑–∏ –∑–Ω–∞–Ω—å?
4. –ß–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∞ (2-4 —Ä–µ—á–µ–Ω–Ω—è)?

–Ø–∫—â–æ –∑–Ω–∞–π–¥–µ–Ω–æ –ø–æ–º–∏–ª–∫–∏ - –≤–∏–ø—Ä–∞–≤ —Ç–∞ —Å—Ñ–æ—Ä–º—É–π –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∑–Ω–æ–≤—É."""
```

---

### 3. –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É

```python
# ollama_optimized/context_optimizer.py
from typing import Dict, List
import re

class ContextOptimizer:
    """–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ç–æ–∫–µ–Ω—ñ–≤"""
    
    MAX_CONTEXT_TOKENS = 2000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ
    
    # –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–∏ —Å–µ–∫—Ü—ñ–π
    SECTION_PRIORITY = {
        "high": ["university", "contacts", "admission.year_2026", "documents"],
        "medium": ["faculties", "tuition", "fields"],
        "low": ["achievements", "international"]
    }
    
    def optimize_context(self, query: str, full_knowledge: Dict) -> Dict:
        """–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∑–∞–ø–∏—Ç—É"""
        # 1. –í–∏–∑–Ω–∞—á–∞—î–º–æ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞
        keywords = self._extract_keywords(query)
        
        # 2. –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ —Å–µ–∫—Ü—ñ—ó
        relevant_sections = self._find_relevant_sections(keywords, full_knowledge)
        
        # 3. –ü—Ä—ñ–æ—Ä–∏—Ç–∏–∑—É—î–º–æ —Å–µ–∫—Ü—ñ—ó
        prioritized = self._prioritize_sections(relevant_sections)
        
        # 4. –û–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä
        optimized = self._limit_context_size(prioritized)
        
        # 5. –ó–∞–≤–∂–¥–∏ –¥–æ–¥–∞—î–º–æ –≤–∞–∂–ª–∏–≤—ñ —Å–µ–∫—Ü—ñ—ó
        optimized["university"] = full_knowledge.get("university", {})
        optimized["contacts"] = full_knowledge.get("contacts", {})
        
        return optimized
    
    def _extract_keywords(self, query: str) -> List[str]:
        """–í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –∑ –∑–∞–ø–∏—Ç—É"""
        # –í–∏–¥–∞–ª—è—î–º–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {"—è–∫", "—â–æ", "–¥–µ", "–∫–æ–ª–∏", "—á–∏", "–¥–ª—è", "–ø—Ä–æ", "–Ω–∞", "–≤", "–∑"}
        
        words = re.findall(r'\b\w+\b', query.lower())
        keywords = [w for w in words if w not in stop_words and len(w) > 2]
        
        return keywords
    
    def _find_relevant_sections(self, keywords: List[str], knowledge: Dict) -> Dict:
        """–ü–æ—à—É–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —Å–µ–∫—Ü—ñ–π"""
        relevant = {}
        
        for section_key, section_data in knowledge.items():
            section_str = str(section_data).lower()
            
            # –†–∞—Ö—É—î–º–æ –∑–±—ñ–≥–∏ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
            matches = sum(1 for kw in keywords if kw in section_str)
            
            if matches > 0:
                relevant[section_key] = {
                    "data": section_data,
                    "relevance_score": matches / len(keywords) if keywords else 0
                }
        
        return relevant
    
    def _prioritize_sections(self, sections: Dict) -> Dict:
        """–ü—Ä—ñ–æ—Ä–∏—Ç–∏–∑–∞—Ü—ñ—è —Å–µ–∫—Ü—ñ–π"""
        prioritized = {}
        
        # –°–ø–æ—á–∞—Ç–∫—É –¥–æ–¥–∞—î–º–æ —Å–µ–∫—Ü—ñ—ó –∑ –≤–∏—Å–æ–∫–∏–º –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–æ–º
        for priority_level in ["high", "medium", "low"]:
            for section_key in self.SECTION_PRIORITY[priority_level]:
                if section_key in sections:
                    prioritized[section_key] = sections[section_key]["data"]
        
        # –ü–æ—Ç—ñ–º –¥–æ–¥–∞—î–º–æ —ñ–Ω—à—ñ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ —Å–µ–∫—Ü—ñ—ó
        for section_key, section_info in sections.items():
            if section_key not in prioritized:
                prioritized[section_key] = section_info["data"]
        
        return prioritized
    
    def _limit_context_size(self, context: Dict) -> Dict:
        """–û–±–º–µ–∂–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"""
        import json
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–º—ñ—Ä
        context_str = json.dumps(context, ensure_ascii=False)
        estimated_tokens = len(context_str) // 4  # –ü—Ä–∏–±–ª–∏–∑–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
        
        if estimated_tokens <= self.MAX_CONTEXT_TOKENS:
            return context
        
        # –Ø–∫—â–æ –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–∏–π - –æ–±—Ä—ñ–∑–∞—î–º–æ –Ω–∏–∑—å–∫–æ–ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–Ω—ñ —Å–µ–∫—Ü—ñ—ó
        limited = {}
        current_tokens = 0
        
        for priority_level in ["high", "medium", "low"]:
            for section_key in self.SECTION_PRIORITY[priority_level]:
                if section_key in context:
                    section_str = json.dumps(context[section_key], ensure_ascii=False)
                    section_tokens = len(section_str) // 4
                    
                    if current_tokens + section_tokens <= self.MAX_CONTEXT_TOKENS:
                        limited[section_key] = context[section_key]
                        current_tokens += section_tokens
                    else:
                        break
        
        return limited
```

---

### 4. –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –ø–∏—Ç–∞–Ω—å

```python
# ollama_optimized/question_classifier.py
import re
from typing import Dict

class QuestionClassifier:
    """–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –ø–∏—Ç–∞–Ω—å –¥–ª—è –≤–∏–±–æ—Ä—É –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—ó —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó"""
    
    QUESTION_PATTERNS = {
        "factual": [
            r"—è–∫—ñ\s+—î", r"—â–æ\s+—Ç–∞–∫–µ", r"–¥–µ\s+–∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è",
            r"—Å–∫—ñ–ª—å–∫–∏\s+—î", r"—è–∫—ñ\s+—Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç—ñ"
        ],
        "comparison": [
            r"–ø–æ—Ä—ñ–≤–Ω—è–π", r"–≤\s+—á–æ–º—É\s+—Ä—ñ–∑–Ω–∏—Ü—è", r"—â–æ\s+–∫—Ä–∞—â–µ",
            r"—è–∫–∞\s+—Ä—ñ–∑–Ω–∏—Ü—è", r"—Å–∫—ñ–ª—å–∫–∏\s+—Ä—ñ–∑–Ω–∏—Ö"
        ],
        "procedural": [
            r"—è–∫\s+–ø–æ–¥–∞—Ç–∏", r"—è–∫—ñ\s+–∫—Ä–æ–∫–∏", r"—â–æ\s+–ø–æ—Ç—Ä—ñ–±–Ω–æ\s+–∑—Ä–æ–±–∏—Ç–∏",
            r"—è–∫\s+–≤—Å—Ç—É–ø–∏—Ç–∏", r"—è–∫\s+–ø—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏—Å—è"
        ],
        "admission": [
            r"–≤—Å—Ç—É–ø", r"–Ω–º—Ç", r"–¥–æ–∫—É–º–µ–Ω—Ç", r"–∫–∞–º–ø–∞–Ω—ñ—è",
            r"–ø—Ä–∞–≤–∏–ª–∞\s+–≤—Å—Ç—É–ø—É", r"—Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ—ó"
        ],
        "tuition": [
            r"–≤–∞—Ä—Ç—ñ—Å—Ç—å", r"—Ü—ñ–Ω–∞", r"—Å–∫—ñ–ª—å–∫–∏\s+–∫–æ—à—Ç—É—î",
            r"–æ–ø–ª–∞—Ç–∞", r"—Ç–∞—Ä–∏—Ñ–∏"
        ],
        "faculties": [
            r"—Ñ–∞–∫—É–ª—å—Ç–µ—Ç", r"—Å–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ—Å—Ç—å", r"–Ω–∞–ø—Ä—è–º",
            r"–æ—Å–≤—ñ—Ç–Ω—ñ\s+–ø—Ä–æ–≥—Ä–∞–º–∏"
        ]
    }
    
    def classify(self, query: str) -> str:
        """–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –ø–∏—Ç–∞–Ω–Ω—è"""
        query_lower = query.lower()
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ —Ç–∏–ø–∏ (–≤ –ø–æ—Ä—è–¥–∫—É –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç—É)
        for q_type, patterns in self.QUESTION_PATTERNS.items():
            for pattern in patterns:
                if re.search(pattern, query_lower):
                    return q_type
        
        # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º - —Ñ–∞–∫—Ç–∏—á–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è
        return "factual"
    
    def get_confidence(self, query: str, question_type: str) -> float:
        """–û—Ü—ñ–Ω–∫–∞ –≤–ø–µ–≤–Ω–µ–Ω–æ—Å—Ç—ñ –≤ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó"""
        query_lower = query.lower()
        patterns = self.QUESTION_PATTERNS.get(question_type, [])
        
        matches = sum(1 for pattern in patterns if re.search(pattern, query_lower))
        total_patterns = len(patterns)
        
        return matches / total_patterns if total_patterns > 0 else 0.0
```

---

### 5. –ö–µ—à—É–≤–∞–Ω–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π

```python
# ollama_optimized/cache.py
import hashlib
import json
from typing import Optional, Dict
from datetime import datetime, timedelta

class ResponseCache:
    """–ö–µ—à—É–≤–∞–Ω–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π –¥–ª—è —à–≤–∏–¥—à–æ–≥–æ –¥–æ—Å—Ç—É–ø—É"""
    
    def __init__(self, max_size: int = 200, ttl_hours: int = 24):
        self.cache: Dict[str, Dict] = {}
        self.max_size = max_size
        self.ttl = timedelta(hours=ttl_hours)
    
    def _get_cache_key(self, query: str, context_hash: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–ª—é—á–∞ –∫–µ—à—É"""
        normalized_query = self._normalize_query(query)
        return hashlib.md5(
            f"{normalized_query}:{context_hash}".encode()
        ).hexdigest()
    
    def _normalize_query(self, query: str) -> str:
        """–ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –∑–∞–ø–∏—Ç—É –¥–ª—è –∫–µ—à—É"""
        # –ü—Ä–∏–≤–æ–¥–∏–º–æ –¥–æ –Ω–∏–∂–Ω—å–æ–≥–æ —Ä–µ–≥—ñ—Å—Ç—Ä—É
        normalized = query.lower().strip()
        
        # –í–∏–¥–∞–ª—è—î–º–æ –∑–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏
        normalized = re.sub(r'\s+', ' ', normalized)
        
        # –í–∏–¥–∞–ª—è—î–º–æ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—é (–∫—Ä—ñ–º –ø–∏—Ç–∞–ª—å–Ω–∏—Ö –∑–Ω–∞–∫—ñ–≤)
        normalized = re.sub(r'[^\w\s?]', '', normalized)
        
        # –°–æ—Ä—Ç—É—î–º–æ —Å–ª–æ–≤–∞ –¥–ª—è –æ–¥–Ω–∞–∫–æ–≤–∏—Ö –ø–∏—Ç–∞–Ω—å –∑ —Ä—ñ–∑–Ω–∏–º –ø–æ—Ä—è–¥–∫–æ–º
        words = normalized.split()
        return ' '.join(sorted(set(words)))  # –í–∏–¥–∞–ª—è—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏
    
    def get(self, query: str, context: Dict) -> Optional[str]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –∑ –∫–µ—à—É"""
        context_hash = hashlib.md5(
            json.dumps(context, sort_keys=True).encode()
        ).hexdigest()
        
        key = self._get_cache_key(query, context_hash)
        
        if key in self.cache:
            entry = self.cache[key]
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ TTL
            if datetime.now() - entry["timestamp"] < self.ttl:
                return entry["response"]
            else:
                # –í–∏–¥–∞–ª—è—î–º–æ –∑–∞—Å—Ç–∞—Ä—ñ–ª–∏–π –∑–∞–ø–∏—Å
                del self.cache[key]
        
        return None
    
    def set(self, query: str, context: Dict, response: str):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –∫–µ—à"""
        # –Ø–∫—â–æ –∫–µ—à –ø–µ—Ä–µ–ø–æ–≤–Ω–µ–Ω–∏–π - –≤–∏–¥–∞–ª—è—î–º–æ –Ω–∞–π—Å—Ç–∞—Ä—ñ—à–∏–π
        if len(self.cache) >= self.max_size:
            oldest_key = min(
                self.cache.keys(),
                key=lambda k: self.cache[k]["timestamp"]
            )
            del self.cache[oldest_key]
        
        context_hash = hashlib.md5(
            json.dumps(context, sort_keys=True).encode()
        ).hexdigest()
        
        key = self._get_cache_key(query, context_hash)
        
        self.cache[key] = {
            "response": response,
            "timestamp": datetime.now(),
            "query": query
        }
    
    def clear(self):
        """–û—á–∏—â–µ–Ω–Ω—è –∫–µ—à—É"""
        self.cache.clear()
    
    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à—É"""
        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "usage_percent": (len(self.cache) / self.max_size) * 100
        }
```

---

### 6. –ë–∞–≥–∞—Ç–æ—Ä—ñ–≤–Ω–µ–≤–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è

```python
# ollama_optimized/validators/multi_level.py
from typing import List
from dataclasses import dataclass
from validators.response_validator import ResponseValidator, ValidationResult

@dataclass
class ValidationLevel:
    """–†—ñ–≤–µ–Ω—å –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó"""
    name: str
    weight: float
    validator: callable

class MultiLevelValidator:
    """–ë–∞–≥–∞—Ç–æ—Ä—ñ–≤–Ω–µ–≤–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π"""
    
    def __init__(self):
        self.base_validator = ResponseValidator()
        
        # –†—ñ–≤–Ω—ñ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
        self.levels = [
            ValidationLevel(
                name="quick",
                weight=1.0,
                validator=self._quick_validation
            ),
            ValidationLevel(
                name="detailed",
                weight=2.0,
                validator=self._detailed_validation
            ),
            ValidationLevel(
                name="semantic",
                weight=1.5,
                validator=self._semantic_validation
            )
        ]
    
    def validate(self, response: str, query: str) -> ValidationResult:
        """–ë–∞–≥–∞—Ç–æ—Ä—ñ–≤–Ω–µ–≤–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è"""
        all_errors = []
        total_weight = 0
        
        for level in self.levels:
            result = level.validator(response, query)
            if not result.is_valid:
                # –ó–≤–∞–∂—É—î–º–æ –ø–æ–º–∏–ª–∫–∏ –∑–∞ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—é —Ä—ñ–≤–Ω—è
                weighted_errors = [
                    f"[{level.name}] {error}" 
                    for error in result.errors
                ]
                all_errors.extend(weighted_errors)
                total_weight += level.weight
        
        is_valid = len(all_errors) == 0
        error_message = "; ".join(all_errors) if all_errors else ""
        
        return ValidationResult(
            is_valid=is_valid,
            error_message=error_message,
            errors=all_errors
        )
    
    def _quick_validation(self, response: str, query: str) -> ValidationResult:
        """–®–≤–∏–¥–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –ø–æ–º–∏–ª–æ–∫"""
        response_lower = response.lower()
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω—ñ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç–∏
        forbidden = self.base_validator._check_forbidden_universities(response_lower)
        if forbidden:
            return ValidationResult(
                is_valid=False,
                errors=[f"–ó–∞–±–æ—Ä–æ–Ω–µ–Ω–∏–π —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç: {forbidden}"]
            )
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –ø–æ—Ä–æ–∂–Ω—é –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        if not response or len(response.strip()) < 10:
            return ValidationResult(
                is_valid=False,
                errors=["–ü–æ—Ä–æ–∂–Ω—è –∞–±–æ –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å"]
            )
        
        return ValidationResult(is_valid=True)
    
    def _detailed_validation(self, response: str, query: str) -> ValidationResult:
        """–î–µ—Ç–∞–ª—å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞"""
        return self.base_validator.validate(response)
    
    def _semantic_validation(self, response: str, query: str) -> ValidationResult:
        """–°–µ–º–∞–Ω—Ç–∏—á–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ"""
        query_lower = query.lower()
        response_lower = response.lower()
        
        # –í–∏—Ç—è–≥—É—î–º–æ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –∑ –ø–∏—Ç–∞–Ω–Ω—è
        query_keywords = self._extract_keywords(query)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ —É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
        found_keywords = sum(
            1 for kw in query_keywords 
            if kw in response_lower
        )
        
        # –Ø–∫—â–æ –º–µ–Ω—à–µ 30% –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –∑–Ω–∞–π–¥–µ–Ω–æ - –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞
        if query_keywords and found_keywords / len(query_keywords) < 0.3:
            return ValidationResult(
                is_valid=False,
                errors=["–í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è"]
            )
        
        return ValidationResult(is_valid=True)
    
    def _extract_keywords(self, text: str) -> List[str]:
        """–í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤"""
        import re
        stop_words = {"—è–∫", "—â–æ", "–¥–µ", "–∫–æ–ª–∏", "—á–∏", "–¥–ª—è", "–ø—Ä–æ"}
        words = re.findall(r'\b\w+\b', text.lower())
        return [w for w in words if w not in stop_words and len(w) > 2]
```

---

### 7. –ó–±—ñ—Ä –º–µ—Ç—Ä–∏–∫

```python
# ollama_optimized/metrics/collector.py
from typing import Dict, List
from datetime import datetime
from database import db

class MetricsCollector:
    """–ó–±—ñ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —Ä–æ–±–æ—Ç–∏ AI"""
    
    def __init__(self):
        self.metrics = {
            "total_requests": 0,
            "cache_hits": 0,
            "validation_failures": 0,
            "regeneration_count": 0,
            "response_times": [],
            "response_lengths": [],
            "question_types": {},
            "errors_by_type": {}
        }
    
    def record_request(
        self, 
        query: str, 
        response: str, 
        response_time: float, 
        from_cache: bool,
        question_type: str = None,
        validation_passed: bool = True
    ):
        """–ó–∞–ø–∏—Å –º–µ—Ç—Ä–∏–∫ –∑–∞–ø–∏—Ç—É"""
        self.metrics["total_requests"] += 1
        
        if from_cache:
            self.metrics["cache_hits"] += 1
        
        if not validation_passed:
            self.metrics["validation_failures"] += 1
        
        if question_type:
            self.metrics["question_types"][question_type] = \
                self.metrics["question_types"].get(question_type, 0) + 1
        
        self.metrics["response_times"].append(response_time)
        self.metrics["response_lengths"].append(len(response))
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ –ë–î –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        self._save_to_db(query, response, response_time, from_cache, question_type)
    
    def get_statistics(self) -> Dict:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        total = self.metrics["total_requests"]
        
        if total == 0:
            return {}
        
        return {
            "total_requests": total,
            "cache_hit_rate": self.metrics["cache_hits"] / total * 100,
            "validation_failure_rate": self.metrics["validation_failures"] / total * 100,
            "avg_response_time": sum(self.metrics["response_times"]) / len(self.metrics["response_times"]) if self.metrics["response_times"] else 0,
            "avg_response_length": sum(self.metrics["response_lengths"]) / len(self.metrics["response_lengths"]) if self.metrics["response_lengths"] else 0,
            "question_types_distribution": self.metrics["question_types"]
        }
    
    async def _save_to_db(
        self, 
        query: str, 
        response: str, 
        response_time: float,
        from_cache: bool,
        question_type: str
    ):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ –≤ –ë–î"""
        try:
            # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∞–±–ª–∏—Ü—é –¥–ª—è –º–µ—Ç—Ä–∏–∫ (—è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î)
            async with db.pool.acquire() as conn:
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS ai_metrics (
                        id SERIAL PRIMARY KEY,
                        query TEXT,
                        response TEXT,
                        response_time FLOAT,
                        from_cache BOOLEAN,
                        question_type VARCHAR(50),
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –º–µ—Ç—Ä–∏–∫–∏
                await conn.execute("""
                    INSERT INTO ai_metrics (query, response, response_time, from_cache, question_type)
                    VALUES ($1, $2, $3, $4, $5)
                """, query[:500], response[:1000], response_time, from_cache, question_type)
        except Exception as e:
            # –õ–æ–≥—É—î–º–æ –ø–æ–º–∏–ª–∫—É, –∞–ª–µ –Ω–µ –∑—É–ø–∏–Ω—è—î–º–æ —Ä–æ–±–æ—Ç—É
            import logging
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫: {e}")
```

---

## üîÑ –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –∑ —ñ—Å–Ω—É—é—á–∏–º –∫–æ–¥–æ–º

### –û–Ω–æ–≤–ª–µ–Ω–Ω—è `ollama_client.py`:

```python
# ollama_client.py (–æ–Ω–æ–≤–ª–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è)
from ollama_optimized.client import OptimizedOllamaClient

# –ó–∞–º—ñ–Ω—é—î–º–æ —Å—Ç–∞—Ä–∏–π –∫–ª—ñ—î–Ω—Ç
ollama = OptimizedOllamaClient()

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è —Ç–∞–∫–∏–º —Å–∞–º–∏–º
response = await ollama.generate_response(user_message, context_list)
```

---

## üìä –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è

### Unit-—Ç–µ—Å—Ç–∏:

```python
# tests/test_ollama_optimized.py
import pytest
from ollama_optimized.client import OptimizedOllamaClient
from ollama_optimized.question_classifier import QuestionClassifier

def test_question_classification():
    classifier = QuestionClassifier()
    
    assert classifier.classify("–Ø–∫—ñ —î —Ñ–∞–∫—É–ª—å—Ç–µ—Ç–∏?") == "faculties"
    assert classifier.classify("–°–∫—ñ–ª—å–∫–∏ –∫–æ—à—Ç—É—î –Ω–∞–≤—á–∞–Ω–Ω—è?") == "tuition"
    assert classifier.classify("–Ø–∫ –≤—Å—Ç—É–ø–∏—Ç–∏?") == "procedural"

def test_cache():
    cache = ResponseCache()
    query = "–Ø–∫—ñ —î —Ñ–∞–∫—É–ª—å—Ç–µ—Ç–∏?"
    context = {"faculties": {}}
    response = "–í –•–î–£ —î 8 —Ñ–∞–∫—É–ª—å—Ç–µ—Ç—ñ–≤..."
    
    cache.set(query, context, response)
    assert cache.get(query, context) == response
```

---

## üöÄ –ü–æ—á–∞—Ç–æ–∫ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

1. **–°—Ç–≤–æ—Ä—ñ—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫:**
```bash
mkdir -p ollama_optimized/validators
mkdir -p ollama_optimized/metrics
mkdir -p ollama_optimized/handlers
```

2. **–°—Ç–≤–æ—Ä—ñ—Ç—å —Ñ–∞–π–ª–∏** –∑–≥—ñ–¥–Ω–æ –∑ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏ –≤–∏—â–µ

3. **–û–Ω–æ–≤—ñ—Ç—å —ñ–º–ø–æ—Ä—Ç–∏** –≤ `main.py` —Ç–∞ `handlers.py`

4. **–ó–∞–ø—É—Å—Ç—ñ—Ç—å —Ç–µ—Å—Ç–∏** –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏

5. **–ú–æ–Ω—ñ—Ç–æ—Ä—å—Ç–µ –º–µ—Ç—Ä–∏–∫–∏** –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –ø–æ–∫—Ä–∞—â–µ–Ω—å

---

**–ì–æ—Ç–æ–≤–æ –¥–æ —ñ–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—ó!** üéâ

